{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import statistics as stat\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer,TfidfTransformer\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score,confusion_matrix,accuracy_score,f1_score,precision_score,recall_score,precision_recall_fscore_support,classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "stopwords = stopwords.words('english')\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAPATH = \"../data/ProtestNews2019/\"\n",
    "stopwords = open(\"../data/stopwords.txt\",'r').read().split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(list_of_articles):\n",
    "    data = [[article['id'],article['url'],' '.join(article['text'].split('\\n')),article['label']] for article in list_of_articles]\n",
    "    return pd.DataFrame(data,columns=['id','url','text','label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "with open(DATAPATH+'all_train_with_org.json', 'rb') as f:\n",
    "        for line in f:\n",
    "            d = json.loads(line)\n",
    "            train_data.append(d)\n",
    "            \n",
    "dev_data = []\n",
    "with open(DATAPATH+'all_dev_with_org.json', 'rb') as f:\n",
    "        for line in f:\n",
    "            d = json.loads(line)\n",
    "            dev_data.append(d)\n",
    "            \n",
    "test_data = []\n",
    "with open(DATAPATH+'all_test_with_org.json', 'rb') as f:\n",
    "        for line in f:\n",
    "            d = json.loads(line)\n",
    "            test_data.append(d)\n",
    "\n",
    "china_data = []\n",
    "with open(DATAPATH+'china_test_with_org.json', 'rb') as f:\n",
    "        for line in f:\n",
    "            d = json.loads(line)\n",
    "            china_data.append(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sampling: 30% of Each Articles' Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(news):\n",
    "    l = WordNetLemmatizer()\n",
    "    sentences = news.split(\".\")\n",
    "    return \" \".join([l.lemmatize(word.lower()) for sentence in sentences for word in sentence.split() \\\n",
    "                     if word not in stopwords if word.isalpha() if len(word)> 2 if word.lower() not in\\\n",
    "                     [\"said\",\"the\",\"first\",\"also\",\"would\",\"one\",\"two\",\"they\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_30(news):\n",
    "    l = WordNetLemmatizer()\n",
    "    sentences = news.split(\".\")\n",
    "    words = \" \".join([l.lemmatize(word.lower()) for sentence in sentences for word in sentence.split() \\\n",
    "                     if word not in stopwords if word.isalpha() if len(word)> 2 if word.lower() not in \\\n",
    "                     [\"said\",\"the\",\"first\",\"also\",\"would\",\"one\",\"two\",\"they\"]])\n",
    "    \n",
    "    return \" \".join(random.sample(words.split(),round(len(words.split())*0.3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = create_dataframe(train_data)\n",
    "df_dev = create_dataframe(dev_data)\n",
    "df_test = create_dataframe(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = df_train.iloc[:,-2:]\n",
    "train['text_30'] = train['text'].map(preprocess_30)\n",
    "train['text'] = train['text'].map(preprocess)\n",
    "\n",
    "dev = df_dev.iloc[:,-2:]\n",
    "dev['text_30'] = dev['text'].map(preprocess_30)\n",
    "dev['text'] = dev['text'].map(preprocess)\n",
    "\n",
    "test = df_test.iloc[:,-2:]\n",
    "test['text_30'] = test['text'].map(preprocess_30)\n",
    "test['text'] = test['text'].map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TfIdf Vectorizer returns a sparse matrix and GaussianNB() takes only dense matrices, so I am using an transformer.\n",
    "class DenseTransformer(TransformerMixin):\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    def transform(self, X, y=None, **fit_params):\n",
    "        return X.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 22s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7361981315157461"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gnb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "    ('clf', GaussianNB()),\n",
    "])\n",
    "\n",
    "hyperparameters = dict(\n",
    "    tfidf__min_df      = (25,50,100),\n",
    "    tfidf__ngram_range = ((1, 1), (1, 2), (1, 3))\n",
    ")\n",
    "\n",
    "gnb_grid_search = GridSearchCV(gnb_pipeline, hyperparameters,cv=3,scoring='f1_macro')\n",
    "\n",
    "gnb_grid_search.fit(train.text, list(train.label))\n",
    "\n",
    "gnb_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF DEV SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.79      0.95      0.87       296\n",
      "         1.0       0.86      0.55      0.67       161\n",
      "\n",
      "    accuracy                           0.81       457\n",
      "   macro avg       0.83      0.75      0.77       457\n",
      "weighted avg       0.82      0.81      0.80       457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASSIFICATION REPORT OF DEV SET:\")\n",
    "print(classification_report(gnb_grid_search.predict(dev.text),dev.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.93      0.85       444\n",
      "         1.0       0.81      0.51      0.62       243\n",
      "\n",
      "    accuracy                           0.78       687\n",
      "   macro avg       0.79      0.72      0.74       687\n",
      "weighted avg       0.79      0.78      0.77       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASSIFICATION REPORT OF TEST SET:\")\n",
    "print(classification_report(gnb_grid_search.predict(test.text),test.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6338851828895108"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "gnb_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('to_dense', DenseTransformer()),\n",
    "    ('clf', GaussianNB()),\n",
    "])\n",
    "\n",
    "hyperparameters = dict(\n",
    "    tfidf__min_df      = (25,50,100),\n",
    "    tfidf__ngram_range = ((1, 1), (1, 2), (1, 3))\n",
    ")\n",
    "\n",
    "gnb_grid_search = GridSearchCV(gnb_pipeline, hyperparameters,cv=3,scoring='f1_macro')\n",
    "\n",
    "gnb_grid_search.fit(train.text_30, list(train.label))\n",
    "\n",
    "gnb_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF DEV SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.62      0.94      0.75       235\n",
      "         1.0       0.85      0.39      0.54       222\n",
      "\n",
      "    accuracy                           0.67       457\n",
      "   macro avg       0.74      0.66      0.64       457\n",
      "weighted avg       0.73      0.67      0.64       457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASSIFICATION REPORT OF DEV SET:\")\n",
    "print(classification_report(gnb_grid_search.predict(dev.text_30),dev.label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLASSIFICATION REPORT OF TEST SET:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.65      0.93      0.77       373\n",
      "         1.0       0.83      0.41      0.55       314\n",
      "\n",
      "    accuracy                           0.69       687\n",
      "   macro avg       0.74      0.67      0.66       687\n",
      "weighted avg       0.73      0.69      0.67       687\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"CLASSIFICATION REPORT OF TEST SET:\")\n",
    "print(classification_report(gnb_grid_search.predict(test.text_30),test.label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 1s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8356848145705001"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "hyperparameters = dict(\n",
    "    tfidf__min_df      = (4, 10, 16),\n",
    "    tfidf__ngram_range = ((1, 1), (1, 2), (1, 3)),\n",
    "    clf__kernel        = [\"linear\",\"sigmoid\"],\n",
    "    clf__C             = np.logspace(1,3,3)\n",
    "\n",
    ")\n",
    "\n",
    "svc_grid_search = GridSearchCV(svc_pipeline, hyperparameters,cv=2,scoring='f1_macro')\n",
    "\n",
    "svc_grid_search.fit(train.text, list(train.label))\n",
    "\n",
    "svc_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 3min 3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.755141174356247"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "svc_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', SVC()),\n",
    "])\n",
    "\n",
    "hyperparameters = dict(\n",
    "    tfidf__min_df      = (4, 10, 16),\n",
    "    tfidf__ngram_range = ((1, 1), (1, 2), (1, 3)),\n",
    "    clf__kernel        = [\"linear\",\"sigmoid\"],\n",
    "    clf__C             = np.logspace(1,3,3)\n",
    "\n",
    ")\n",
    "\n",
    "svc_grid_search = GridSearchCV(svc_pipeline, hyperparameters,cv=2,scoring='f1_macro')\n",
    "\n",
    "svc_grid_search.fit(train.text_30, list(train.label))\n",
    "\n",
    "svc_grid_search.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
